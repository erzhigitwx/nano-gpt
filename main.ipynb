{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-20T17:26:07.126043Z",
     "start_time": "2026-01-20T17:26:07.122111Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:26:07.324087Z",
     "start_time": "2026-01-20T17:26:07.311011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()\n",
    "\n",
    "letters = list(file)\n",
    "print(len(letters))\n",
    "print(file[:15])"
   ],
   "id": "21fea43e34e63724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:26:01.152715Z",
     "start_time": "2026-01-20T17:26:00.505464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 256\n",
    "d_model = 512\n",
    "num_merges = 200\n",
    "itos = {i: bytes([i]).decode('utf-8', errors='ignore') for i in range(256)}\n",
    "tokens = list(file.encode(\"utf-8\"))\n",
    "\n",
    "vocab_size = vocab_size + num_merges\n",
    "\n",
    "chars = sorted(set(letters))\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "C = token_embedding(torch.tensor(tokens, dtype=torch.long))\n",
    "\n",
    "print(C.shape)"
   ],
   "id": "fedc13dbc48d9c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394, 512])\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:11.576740Z",
     "start_time": "2026-01-20T17:53:11.569453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, new_id):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            new_ids.append(new_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, num_merges=200):\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = OrderedDict()\n",
    "        self.vocab = {}\n",
    "\n",
    "    def train(self, text):\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        for i in range(self.num_merges):\n",
    "            stats = get_stats(tokens)\n",
    "\n",
    "            if not stats:\n",
    "                print(f\"больше нет пар\")\n",
    "                break\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "            new_id = 256 + i\n",
    "\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocab[new_id] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "            tokens = merge(tokens, pair, new_id)\n",
    "\n",
    "        print(f\"   Сжатие: {len(text.encode('utf-8'))} → {len(tokens)} токенов\")\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        for pair, new_id in self.merges.items():\n",
    "            tokens = merge(tokens, pair, new_id)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        byte_array = b''.join([self.vocab[int(token)] for token in tokens])\n",
    "        return byte_array.decode('utf-8', errors='replace')\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)"
   ],
   "id": "3e7bb263b0735d10",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:12.403226Z",
     "start_time": "2026-01-20T17:53:12.399982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ],
   "id": "7c1f5275c530dc5e",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:12.612391Z",
     "start_time": "2026-01-20T17:53:12.555619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "data = torch.tensor(indices, dtype=torch.long)\n",
    "x, y = get_batch(data, block_size=128, batch_size=32) # 32 x 128\n",
    "x_emb = token_embedding(x) # 32 x 128 x 512\n",
    "pos_enc = positional_encoding(128, 512)\n",
    "x_emb = x_emb + pos_enc"
   ],
   "id": "a22c85f3eaf387b0",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:12.740508Z",
     "start_time": "2026-01-20T17:53:12.735768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None): # x.shape is (32 x 128 x 512)\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k) # (32 x 128 x 8 x 64)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2) # (32 x 8 x 128 x 64)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k) # (32 x 8 x 128 x 64) @ (32 x 8 x 64 x 128) -> (32 x 8 x 128 x 128)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ V # (32 x 8 x 128 x 128) @ (32 x 8 x 128 x 64) -> (32 x 8 x 128 x 64)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return self.W_o(out)"
   ],
   "id": "b1f9a372cfa32f73",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:12.906496Z",
     "start_time": "2026-01-20T17:53:12.902946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x\n"
   ],
   "id": "89931f7481d4f154",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:13.109142Z",
     "start_time": "2026-01-20T17:53:13.103868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.masked_attn(x, mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x\n"
   ],
   "id": "b0cb8b00e39fc591",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:13.287973Z",
     "start_time": "2026-01-20T17:53:13.283593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    print(mask)\n",
    "    mask = mask == 0\n",
    "    return mask\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([itos[int(i)] for i in indices])\n"
   ],
   "id": "5750e3187428e272",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:53:13.470697Z",
     "start_time": "2026-01-20T17:53:13.464740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            create_causal_mask(block_size).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(x) # (B, T, d_model)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb # (B, T, d_model)\n",
    "\n",
    "        mask = self.causal_mask[:, :, :T, :T]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n"
   ],
   "id": "7abc0c854118f294",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T18:03:13.700921Z",
     "start_time": "2026-01-20T17:53:13.646982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_merges = 200\n",
    "    tokenizer = BPETokenizer(num_merges=num_merges)\n",
    "    train_tokens = tokenizer.train(file)\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = d_model * 4\n",
    "    block_size = 128\n",
    "\n",
    "    model = GPT(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        d_ff=d_ff,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Модель создана:\")\n",
    "    print(f\"  Словарь: {vocab_size} токенов\")\n",
    "    print(f\"  Параметры: {total_params:,}\")\n",
    "    print(f\"  Block size: {block_size}\")\n",
    "    print()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print()\n",
    "\n",
    "    num_epochs = 10\n",
    "    steps_per_epoch = 1000\n",
    "    eval_interval = 500\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            x, y = get_batch(torch.tensor(train_tokens, dtype=torch.long).to(device), block_size, batch_size)\n",
    "\n",
    "            logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            y = y.view(B * T)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % eval_interval == 0:\n",
    "                print(f\"Epoch {epoch}, Step {step:4d}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ГЕНЕРАЦИЯ (конец эпохи {epoch})\")\n",
    "        print(\"=\" * 70)\n",
    "        model.eval()\n",
    "\n",
    "        prompts = [\"The\", \"In\", \"Kazakhstan\"]\n",
    "\n",
    "        for prompt in prompts:\n",
    "            context = tokenizer.encode(prompt)\n",
    "            context = torch.tensor([context], dtype=torch.long, device=device)\n",
    "\n",
    "            generated = model.generate(\n",
    "                context,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "\n",
    "            generated_text = tokenizer.decode(generated[0])\n",
    "            print(f\"Generated: {generated_text}\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "    print(\"Обучение завершено!\")"
   ],
   "id": "7b66405900fbfb80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Сжатие: 1115394 → 598357 токенов\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Модель создана:\n",
      "  Словарь: 456 токенов\n",
      "  Параметры: 19,448,264\n",
      "  Block size: 128\n",
      "\n",
      "\n",
      "Epoch 0, Step    0, Loss: 6.2947\n",
      "Epoch 0, Step  500, Loss: 2.7849\n",
      "\n",
      "======================================================================\n",
      "ГЕНЕРАЦИЯ (конец эпохи 0)\n",
      "======================================================================\n",
      "Generated: Thee will make our truth, fand, stranken,\n",
      "To miss to faces urged as the violate,\n",
      "With assaufice to wind's grave.\n",
      "To revenge that he determotion ewhen 'tle!\n",
      "\n",
      "KINGHENRY VI:\n",
      "Why, blood and se\n",
      "----------------------------------------------------------------------\n",
      "Generated: In thri; and, lieu\n",
      "And be report. If he cannot to do at\n",
      "defamine of the law of pamitt need.\n",
      "\n",
      "Nurse:\n",
      "Though my peace are not wast burder where were the faith chor\n",
      "To quicklethes: andeth had my fin\n",
      "----------------------------------------------------------------------\n",
      "Generated: Kazakhstant gust remains\n",
      "There's heath, he pleasure tate late, my lord.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Plirtuady PETRUCHuntspetering wack, it inking hand feels! it is\n",
      "Was thanks, nor youn garden\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Step    0, Loss: 2.3964\n",
      "Epoch 1, Step  500, Loss: 2.0111\n",
      "\n",
      "======================================================================\n",
      "ГЕНЕРАЦИЯ (конец эпохи 1)\n",
      "======================================================================\n",
      "Generated: Thee will endrums;\n",
      "I and the things that to beseep upon thee,\n",
      "They shall be ross, Bolingbroke Harry of Buckingham.\n",
      "\n",
      "KING RICHARD III:\n",
      "Assemble you: what rests he hath made\n",
      "Just all admit him to \n",
      "----------------------------------------------------------------------\n",
      "Generated: In threatenings.\n",
      "\n",
      "ANTONIO:\n",
      "He's at a devil, I see thy chamber, look up!\n",
      "As you will prove your father name!\n",
      "About such as heavy monstrous in a taught,\n",
      "Part up such villains, d\n",
      "----------------------------------------------------------------------\n",
      "Generated: Kazakhstant'd and time offence\n",
      "Stare let harm a little back'd watery,\n",
      "To look him that ot o' the centre walls of prey,\n",
      "And made it not again.\n",
      "\n",
      "Assist:\n",
      "Thou wilt ow. What is it, for there many joy\n",
      "But ordth \n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "Epoch 2, Step    0, Loss: 1.5852\n",
      "Epoch 2, Step  500, Loss: 1.3409\n",
      "\n",
      "======================================================================\n",
      "ГЕНЕРАЦИЯ (конец эпохи 2)\n",
      "======================================================================\n",
      "Generated: They shall I wot on Rome;\n",
      "This is night Tyba' land for shall hither;\n",
      "Thou, hast needle baM to fallen be sun,\n",
      "From all damned'st thou, Your brother is the greatest gentlew\n",
      "To use his brother come. A\n",
      "----------------------------------------------------------------------\n",
      "Generated: In perishold; no sure of doth he girl.\n",
      "Dear father, and I think, is come at htly.\n",
      "Now my son's the matter blood up all\n",
      "Is loath to me, and my father's love for Ireign,\n",
      "Since I never will confess to this king ha\n",
      "----------------------------------------------------------------------\n",
      "Generated: Kazakhstant of heaven,\n",
      "And both dishonour.\n",
      "\n",
      "HASTINGS:\n",
      "Margaret!\n",
      "\n",
      "Messenger:\n",
      "Upon the which he doth chastised? What\n",
      "time and hence the blood of Lancaster,\n",
      "With smile and her love? and your mispost.\n",
      "\n",
      "LA\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "Epoch 3, Step    0, Loss: 0.9390\n",
      "Epoch 3, Step  500, Loss: 0.6299\n",
      "\n",
      "======================================================================\n",
      "ГЕНЕРАЦИЯ (конец эпохи 3)\n",
      "======================================================================\n",
      "Generated: They--holder, that queen lific!\n",
      "Lie ld entreaties, to yours; you shall know beat you\n",
      "why he hath wisher love for, he shows you farther.\n",
      "\n",
      "DERBY:\n",
      "Good LEents, Friar, who with me dought?\n",
      "and hear me sp\n",
      "----------------------------------------------------------------------\n",
      "Generated: In Seeford may you pass and queen these wrongs\n",
      "Thrust for Warwick's grave; or, by here.\n",
      "\n",
      "GREMIO:\n",
      "Thou hast or an Edward will tell the link.\n",
      "\n",
      "HORTENSIO:\n",
      "To, tell me what else?\n",
      "\n",
      "GREMIO:\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Generated: Kazakhstant; your most grave her.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Up, where comes my Earl of Yorkby?\n",
      "Richard and Warwick; misdom were �ests be past.\n",
      "\n",
      "WARWICK:\n",
      "Say:\n",
      "What say'st thou? by Jupitoly did I den\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "Epoch 4, Step    0, Loss: 0.4866\n",
      "Epoch 4, Step  500, Loss: 0.3989\n",
      "\n",
      "======================================================================\n",
      "ГЕНЕРАЦИЯ (конец эпохи 4)\n",
      "======================================================================\n",
      "Generated: They have feed and true should consume to speak.\n",
      "\n",
      "CORIOLANUS:\n",
      "O that I can't live: thousand looks\n",
      "Did not to me forthwith.\n",
      "\n",
      "MENENIUS:\n",
      "This is strange now to make you not very late.\n",
      "\n",
      "COMINIUS:\n",
      "Go, muster him.\n",
      "\n",
      "C\n",
      "----------------------------------------------------------------------\n",
      "Generated: In this? I have so, I think, be ignorant,\n",
      "To open him to-morrow or next day:\n",
      "He is within, with jealous and trumpet, kneel'd;\n",
      "Why I am modest on my sineward gentle blothes\n",
      "Last \n",
      "----------------------------------------------------------------------\n",
      "Generated: Kazakhstantly of me--\n",
      "Why then I disdain'd thy servited brow!\n",
      "Curses white begins spices we me: fore-utment\n",
      "I am loathed upon rub, physice\n",
      "To meet at ease; where I have\n",
      "Apo\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "Epoch 5, Step    0, Loss: 0.4296\n",
      "Epoch 5, Step  500, Loss: 0.3402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[91]\u001B[39m\u001B[32m, line 46\u001B[39m\n\u001B[32m     43\u001B[39m model.train()\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(steps_per_epoch):\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     x, y = get_batch(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, block_size, batch_size)\n\u001B[32m     48\u001B[39m     logits = model(x)  \u001B[38;5;66;03m# (B, T, vocab_size)\u001B[39;00m\n\u001B[32m     50\u001B[39m     B, T, C = logits.shape\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T18:03:19.378917Z",
     "start_time": "2026-01-20T18:03:19.375118Z"
    }
   },
   "cell_type": "code",
   "source": "loss.item()",
   "id": "7e5c68f6a2300b83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34625616669654846"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
