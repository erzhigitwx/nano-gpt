{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:11.512966Z",
     "start_time": "2026-01-13T12:03:11.509185Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:12.401064Z",
     "start_time": "2026-01-13T12:03:12.321399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()\n",
    "letters = list(file)\n",
    "print(len(letters))"
   ],
   "id": "21fea43e34e63724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:13.726159Z",
     "start_time": "2026-01-13T12:03:13.161748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(set(letters))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i : ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(stoi)\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "d_model = 512\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "indices = [stoi[ch] for ch in letters]\n",
    "C = token_embedding(torch.tensor(indices, dtype=torch.long))\n",
    "print(C.shape)"
   ],
   "id": "fedc13dbc48d9c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "torch.Size([1115394, 512])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:14.452802Z",
     "start_time": "2026-01-13T12:03:14.449112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ],
   "id": "7c1f5275c530dc5e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:15.546194Z",
     "start_time": "2026-01-13T12:03:15.435686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "data = torch.tensor(indices, dtype=torch.long)\n",
    "x, y = get_batch(data, block_size=128, batch_size=32) # 32 x 128\n",
    "x_emb = token_embedding(x) # 32 x 128 x 512\n",
    "pos_enc = positional_encoding(128, 512)\n",
    "x_emb = x_emb + pos_enc"
   ],
   "id": "a22c85f3eaf387b0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:04:18.925461Z",
     "start_time": "2026-01-13T12:04:18.919296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None): # x.shape is (32 x 128 x 512)\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k) # (32 x 128 x 8 x 64)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2) # (32 x 8 x 128 x 64)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k) # (32 x 8 x 128 x 64) @ (32 x 8 x 64 x 128) -> (32 x 8 x 128 x 128)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ V # (32 x 8 x 128 x 128) @ (32 x 8 x 128 x 64) -> (32 x 8 x 128 x 64)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return self.W_o(out)"
   ],
   "id": "b1f9a372cfa32f73",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:16.931784Z",
     "start_time": "2026-01-13T12:03:16.927555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x\n"
   ],
   "id": "89931f7481d4f154",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:18.235801Z",
     "start_time": "2026-01-13T12:03:18.231936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.masked_attn(x, mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x\n"
   ],
   "id": "b0cb8b00e39fc591",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:13:04.503730Z",
     "start_time": "2026-01-13T12:13:04.500235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    print(mask)\n",
    "    mask = mask == 0\n",
    "    return mask\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([itos[int(i)] for i in indices])"
   ],
   "id": "5750e3187428e272",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:03:22.108350Z",
     "start_time": "2026-01-13T12:03:22.102619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            create_causal_mask(block_size).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(x) # (B, T, d_model)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb # (B, T, d_model)\n",
    "\n",
    "        mask = self.causal_mask[:, :, :T, :T]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n"
   ],
   "id": "7abc0c854118f294",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:28:41.925727Z",
     "start_time": "2026-01-13T12:13:12.553628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    d_ff=2048,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "eval_interval = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(1000):\n",
    "        x, y = get_batch(data, block_size=128, batch_size=32)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x) # (32 x 128 x 65)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        y = y.view(-1)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\n=== Generation ====\")\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=200)\n",
    "\n",
    "    print(decode(generated[0]))\n",
    "    print(\"=\" * 50 + \"\\n\")\n"
   ],
   "id": "7b66405900fbfb80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0, Step 0, Loss: 4.3087\n",
      "Epoch 0, Step 500, Loss: 1.7103\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "\n",
      "BALTHASTIO:\n",
      "My lords, I did them not no mark you give\n",
      "The shine of heard to hear heards and these was\n",
      "I sexcurtuted him it deper as one seving\n",
      "Will my follour'd pleacent wiced beas look.\n",
      "\n",
      "Bless Of Gl\n",
      "==================================================\n",
      "\n",
      "Epoch 1, Step 0, Loss: 1.4198\n",
      "Epoch 1, Step 500, Loss: 1.3780\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "Would up he time were thou wilt profare\n",
      "Your cabins their partiest. Then, may well, are he--\n",
      "\n",
      "BRUTUS:\n",
      "And all rest thou like wanteding diver\n",
      "To Eithor they have at the poor mother?\n",
      "\n",
      "BENVOLIO:\n",
      "'Tis lea\n",
      "==================================================\n",
      "\n",
      "Epoch 2, Step 0, Loss: 1.3015\n",
      "Epoch 2, Step 500, Loss: 1.1628\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "YORK:\n",
      "Therein ralm'd from effect not him to thee hence!\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "GLOUCESTER:\n",
      "If, read for me.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Why, Camillo,--\n",
      "\n",
      "LUCIO:\n",
      "My name rest is will lady and 'gainst,'\n",
      "Hath lookest nock my\n",
      "==================================================\n",
      "\n",
      "Epoch 3, Step 0, Loss: 1.1508\n",
      "Epoch 3, Step 500, Loss: 1.0776\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "QUEN LADY MONTAGUE:\n",
      "So they she's do; and so reports no thousand dear\n",
      "When best thou bring the like to Richard's ground:\n",
      "Tell his men hardly venom'd,\n",
      "Nor one that thou hast a French'd with too,\n",
      "Which \n",
      "==================================================\n",
      "\n",
      "Epoch 4, Step 0, Loss: 1.0580\n",
      "Epoch 4, Step 500, Loss: 0.9555\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "Second sad a healter way to France,\n",
      "And I the midnight to sore my joy\n",
      "Rider hand together: he shall burn the buyer of his eyes,\n",
      "Accase his sworn be warrain 'gaged cures:\n",
      "And the fault got that ways fa\n",
      "==================================================\n",
      "\n",
      "Epoch 5, Step 0, Loss: 0.8348\n",
      "Epoch 5, Step 500, Loss: 0.7566\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "\n",
      "WARWICK:\n",
      "Salunes him his charity, with all gross spirit.\n",
      "\n",
      "KING LEWIS RICHARD III:\n",
      "Faith, for he first it.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "But thou to joy; lie thee enemy to hito.\n",
      "\n",
      "WARWICK:\n",
      "Sweet rest his suit will \n",
      "==================================================\n",
      "\n",
      "Epoch 6, Step 0, Loss: 0.6961\n",
      "Epoch 6, Step 500, Loss: 0.6624\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Nor never till death, my lord,\n",
      "Live, in my brain, a penitent trade:\n",
      "My state, in brief, a very sin,\n",
      "And spit fit then by the storm of state,\n",
      "Begpara's 'em; and I do think it out of my \n",
      "==================================================\n",
      "\n",
      "Epoch 7, Step 0, Loss: 0.5466\n",
      "Epoch 7, Step 500, Loss: 0.5226\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "MIRANDA:\n",
      "Am I like king? I do not long again.\n",
      "\n",
      "DERBY:\n",
      "Well, well, bring thee to a slave, so evident,\n",
      "I will may be well and be returned:\n",
      "But wherefore I am lute, all commit me do\n",
      "I not know, your fath\n",
      "==================================================\n",
      "\n",
      "Epoch 8, Step 0, Loss: 0.4457\n",
      "Epoch 8, Step 500, Loss: 0.3864\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "And give him greatness! mild, ah, a drum, pair:\n",
      "And if you were as good as breath, that strike\n",
      "Which now the terms of Rome is excell'd\n",
      "The mortal, corruption of a devil\n",
      "Beguile of his country's bjesic\n",
      "==================================================\n",
      "\n",
      "Epoch 9, Step 0, Loss: 0.4395\n",
      "Epoch 9, Step 500, Loss: 0.4014\n",
      "\n",
      "=== Generation ====\n",
      "\n",
      "Me, smear'd; she was come with thee\n",
      "And with safety odds extecholness,\n",
      "To whose flatterers hath clear'd out in some fewlyvorn\n",
      "And as when so discover a few blasting foul minds\n",
      "traitors? as they are ch\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:29:03.985791Z",
     "start_time": "2026-01-13T12:29:03.981969Z"
    }
   },
   "cell_type": "code",
   "source": "loss.item()",
   "id": "7e5c68f6a2300b83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3808194696903229"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:34:01.366536Z",
     "start_time": "2026-01-13T12:34:01.363571Z"
    }
   },
   "cell_type": "code",
   "source": "# trying to make smth with user-input. Of course model is not good enough but we have what we have :)",
   "id": "178a2c0283059ef6",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T12:37:01.548345Z",
     "start_time": "2026-01-13T12:36:59.438456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"tell me about the lord, how kind is he?\"\n",
    "context_indices = torch.tensor([stoi[c] for c in prompt], dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated = model.generate(context_indices, max_new_tokens=300)\n",
    "output_text = decode(generated[0])\n",
    "print(output_text)\n"
   ],
   "id": "79899c79f5de0351",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me about the lord, how kind is he?\n",
      "\n",
      "DUKE OF AUNT:\n",
      "It may be your countrymen, you shall stiff dear this:\n",
      "Then, Petruchio is friending in Marshal:\n",
      "Then all believe me, of Juliet.\n",
      "The valiant heart is not white here;\n",
      "If she plays else you all happiness after byOr this, which, by something approach\n",
      "Hath made to if the jewel. Then he had\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
